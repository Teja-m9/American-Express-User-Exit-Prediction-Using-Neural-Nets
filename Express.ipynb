#importing libraries
import pandas as pd
import numpy as np
import tensorflow as tf

#Check the version of tensorflow
tf.__version__

#Reading the dataset
data=pd.read_csv('/content/American.csv')

data.head()

data.head()
data.dtypes

#Removies the NAN Values
data = data.dropna()

data

#Encoding the values
data=pd.get_dummies(data)

data

#Dividing the independent and dependent features
x_train = data.iloc[:, :-1].values

x_train

y_train = data.iloc[:, -1].values

y_train

#Label Encoding
from sklearn.preprocessing import LabelEncoder
c = LabelEncoder()
y_train = c.fit_transform(y_train)

#Type conversion to Float
x_train = x_train.astype(np.float32)
y_train = y_train.astype(np.float32)

#Splitting the dataset into traininig and testing set
from sklearn.model_selection import train_test_split
x_train1, x_test, y_train1, y_test = train_test_split(x_train, y_train, test_size=0.3, random_state=42)

#Scaling the features
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
x_train1 = sc.fit_transform(x_train1)
x_test = sc.transform(x_test)

#Type checking
x_train1 = x_train1.astype(np.float32)
x_test = x_test.astype(np.float32)

#Model Creation
model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Dense(units=5, activation='relu'))
model.add(tf.keras.layers.Dense(units=5, activation='relu'))
model.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))

#Compilation of the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

#Fitting the model
model.fit(x_train1, y_train1, batch_size=32, epochs=120, validation_data=(x_test, y_test))

#Saving the model for Frontend Frameworks.
model.save('/content/American.csv.h5')


